{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "para clssifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gr8nishan/MLWork/blob/master/para_clssifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-m36De3B_xh",
        "colab_type": "text"
      },
      "source": [
        "Downloading data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7E37H-cCFis",
        "colab_type": "text"
      },
      "source": [
        "Unzipping training and testing file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bca8nEFDCJUs",
        "colab_type": "text"
      },
      "source": [
        "Downloading fast text vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtBKsPeBLB1T",
        "colab_type": "code",
        "outputId": "e78d37f7-f31f-4410-a056-4dc7a939e999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 09:07:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  48.1MB/s    in 31s     \n",
            "\n",
            "2020-02-27 09:07:48 (46.5 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JSyx2zLCM_R",
        "colab_type": "text"
      },
      "source": [
        "Unzipping fast text vecotrs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3A-qORKLYSn",
        "colab_type": "code",
        "outputId": "4bda3d5b-8fd4-49bb-f88f-2768ef28914c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip crawl-300d-2M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  crawl-300d-2M.vec.zip\n",
            "  inflating: crawl-300d-2M.vec       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVCEOiZmCP7C",
        "colab_type": "text"
      },
      "source": [
        "importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmiwR2yiKGUN",
        "colab_type": "code",
        "outputId": "7da7c621-b0e0-4e17-f1d5-8485c72581c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "\n",
        "\n",
        "EMBEDDING_FILE = '/content/crawl-300d-2M.vec'\n",
        "\n",
        "train = pd.read_csv('/content/Output_encoding.csv',encoding=)\n",
        "test = pd.read_csv('/content/Test_Output_encoding.csv')\n",
        "submission = pd.read_csv('/content/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x97 in position 528: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-de63180c278d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/crawl-300d-2M.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Output_encoding.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Test_Output_encoding.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x97 in position 528: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Y_l2RKvJmL",
        "colab_type": "code",
        "outputId": "98b458ff-7f91-43c5-b06c-d5de2fd2dd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohaxAR1O03-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11n8fXovCVWB",
        "colab_type": "text"
      },
      "source": [
        "Converting to a binary classification\n",
        "If all the classes are 0 then the new class **finaltoxicity** will be 0 else if any of the class is 1 then **finaltoxicity** will be 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg86Sxme1p7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.loc[(df.toxic==0) &(df.severe_toxic==0) & (df.obscene==0) & (df.threat==0) & (df.insult==0) & (df.identity_hate==0), 'finaltoxicity'] = 0\n",
        "df.loc[(df.toxic==1) | (df.severe_toxic==1) | (df.obscene==1) | (df.threat==1) | (df.insult==1) | (df.identity_hate==1), 'finaltoxicity'] = 1\n",
        "#df.loc[df.set_of_numbers > 4, 'equal_or_lower_than_4?'] = 'False'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK45Qzn-17Ul",
        "colab_type": "code",
        "outputId": "25786d98-18d0-457c-e5b0-ba3a6d09e7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>finaltoxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... finaltoxicity\n",
              "0  0000997932d777bf  ...           0.0\n",
              "1  000103f0d9cfb60f  ...           0.0\n",
              "2  000113f07ec002fd  ...           0.0\n",
              "3  0001b41b1c6bb37e  ...           0.0\n",
              "4  0001d958c54c6e35  ...           0.0\n",
              "5  00025465d4725e87  ...           0.0\n",
              "6  0002bcb3da6cb337  ...           1.0\n",
              "7  00031b1e95af7921  ...           0.0\n",
              "8  00037261f536c51d  ...           0.0\n",
              "9  00040093b2687caa  ...           0.0\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns5YkKBpCrQE",
        "colab_type": "text"
      },
      "source": [
        "this will give how many rows contains 1 and how many rows contains 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe1HMlwt23R9",
        "colab_type": "code",
        "outputId": "f7ffbfa3-8b82-4a9e-86f4-f524d8edaef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(df['finaltoxicity'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    143346\n",
            "1.0     16225\n",
            "Name: finaltoxicity, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pr2otG4fn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urb4Gp_2C0JE",
        "colab_type": "text"
      },
      "source": [
        "this will rempve punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXeQ8x-k4UMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~.'\n",
        "def remove_punctuation_3(text):\n",
        "  return text.translate(str.maketrans('', '', puncts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWiIvcGZC3VH",
        "colab_type": "text"
      },
      "source": [
        "this will remove all contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_5NGR-J4Xwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aiXM2BLC7II",
        "colab_type": "text"
      },
      "source": [
        "preprocessing for the dataset, replacing few words based on dataset, spelling correction etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUAIrExk5LJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import re\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "\n",
        "class BaseTokenizer(object):\n",
        "    def process_text(self, text):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def process(self, texts):\n",
        "        for text in texts:\n",
        "            yield self.process_text(text)\n",
        "\n",
        "\n",
        "RE_PATTERNS = {\n",
        "    ' american ':\n",
        "        [\n",
        "            'amerikan'\n",
        "        ],\n",
        "\n",
        "    ' adolf ':\n",
        "        [\n",
        "            'adolf'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' hitler ':\n",
        "        [\n",
        "            'hitler'\n",
        "        ],\n",
        "\n",
        "    ' fuck':\n",
        "        [\n",
        "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
        "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
        "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
        "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
        "            'feck ', ' fux ', 'f\\*\\*', \n",
        "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
        "        ],\n",
        "\n",
        "    ' ass ':\n",
        "        [\n",
        "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
        "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
        "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
        "        ],\n",
        "\n",
        "    ' ass hole ':\n",
        "        [\n",
        "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
        "        ],\n",
        "\n",
        "    ' bitch ':\n",
        "        [\n",
        "            'b[w]*i[t]*ch', 'b!tch',\n",
        "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
        "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n",
        "        ],\n",
        "\n",
        "    ' bastard ':\n",
        "        [\n",
        "            'ba[s|z]+t[e|a]+rd'\n",
        "        ],\n",
        "\n",
        "    ' trans gender':\n",
        "        [\n",
        "            'transgender'\n",
        "        ],\n",
        "\n",
        "    ' gay ':\n",
        "        [\n",
        "            'gay'\n",
        "        ],\n",
        "\n",
        "    ' cock ':\n",
        "        [\n",
        "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
        "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
        "        ],\n",
        "\n",
        "    ' dick ':\n",
        "        [\n",
        "            ' dick[^aeiou]', 'deek', 'd i c k'\n",
        "        ],\n",
        "\n",
        "    ' suck ':\n",
        "        [\n",
        "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
        "        ],\n",
        "\n",
        "    ' cunt ':\n",
        "        [\n",
        "            'cunt', 'c u n t'\n",
        "        ],\n",
        "\n",
        "    ' bull shit ':\n",
        "        [\n",
        "            'bullsh\\*t', 'bull\\$hit'\n",
        "        ],\n",
        "\n",
        "    ' homo sex ual':\n",
        "        [\n",
        "            'homosexual'\n",
        "        ],\n",
        "\n",
        "    ' jerk ':\n",
        "        [\n",
        "            'jerk'\n",
        "        ],\n",
        "\n",
        "    ' idiot ':\n",
        "        [\n",
        "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
        "                                                                                      'i d i o t'\n",
        "        ],\n",
        "\n",
        "    ' dumb ':\n",
        "        [\n",
        "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
        "        ],\n",
        "\n",
        "    ' shit ':\n",
        "        [\n",
        "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n",
        "        ],\n",
        "\n",
        "    ' shit hole ':\n",
        "        [\n",
        "            'shythole'\n",
        "        ],\n",
        "\n",
        "    ' retard ':\n",
        "        [\n",
        "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
        "        ],\n",
        "\n",
        "    ' rape ':\n",
        "        [\n",
        "            ' raped'\n",
        "        ],\n",
        "\n",
        "    ' dumb ass':\n",
        "        [\n",
        "            'dumbass', 'dubass'\n",
        "        ],\n",
        "\n",
        "    ' ass head':\n",
        "        [\n",
        "            'butthead'\n",
        "        ],\n",
        "\n",
        "    ' sex ':\n",
        "        [\n",
        "            'sexy', 's3x', 'sexuality'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' nigger ':\n",
        "        [\n",
        "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
        "        ],\n",
        "\n",
        "    ' shut the fuck up':\n",
        "        [\n",
        "            'stfu'\n",
        "        ],\n",
        "\n",
        "    ' pussy ':\n",
        "        [\n",
        "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
        "        ],\n",
        "\n",
        "    ' faggot ':\n",
        "        [\n",
        "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
        "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
        "        ],\n",
        "\n",
        "    ' mother fucker':\n",
        "        [\n",
        "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
        "        ],\n",
        "\n",
        "    ' whore ':\n",
        "        [\n",
        "            'wh\\*\\*\\*', 'w h o r e'\n",
        "        ],\n",
        "}\n",
        "\n",
        "\n",
        "class PatternTokenizer(BaseTokenizer):\n",
        "    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n",
        "                 remove_repetitions=True):\n",
        "        self.lower = lower\n",
        "        self.patterns = patterns\n",
        "        self.initial_filters = initial_filters\n",
        "        self.remove_repetitions = remove_repetitions\n",
        "\n",
        "    def process_text(self, text):\n",
        "        x = self._preprocess(text)\n",
        "        for target, patterns in self.patterns.items():\n",
        "            for pat in patterns:\n",
        "                x = re.sub(pat, target, x)\n",
        "        x = re.sub(r\"[^a-z' ]\", ' ', x)\n",
        "        return x.split()\n",
        "\n",
        "    def process_ds(self, ds):\n",
        "        ### ds = Data series\n",
        "\n",
        "        # lower\n",
        "        ds = copy.deepcopy(ds)\n",
        "        if self.lower:\n",
        "            ds = ds.str.lower()\n",
        "        # remove special chars\n",
        "        if self.initial_filters is not None:\n",
        "            ds = ds.str.replace(self.initial_filters, ' ')\n",
        "        # fuuuuck => fuck\n",
        "        if self.remove_repetitions:\n",
        "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
        "            ds = ds.str.replace(pattern, r\"\\1\")\n",
        "\n",
        "        for target, patterns in self.patterns.items():\n",
        "            for pat in patterns:\n",
        "                ds = ds.str.replace(pat, target)\n",
        "\n",
        "        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n",
        "\n",
        "        return ds.str.split()\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        # lower\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "\n",
        "        # remove special chars\n",
        "        if self.initial_filters is not None:\n",
        "            text = re.sub(self.initial_filters, ' ', text)\n",
        "\n",
        "        # fuuuuck => fuck\n",
        "        if self.remove_repetitions:\n",
        "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
        "            text = pattern.sub(r\"\\1\", text)\n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKrYCxsDDNCy",
        "colab_type": "text"
      },
      "source": [
        "intiializing constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbaPgyWTuJp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 30000\n",
        "maxlen = 100\n",
        "embed_size = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IvxK0-w5wtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN=\"comment_text\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyEweLE7DPOW",
        "colab_type": "text"
      },
      "source": [
        "preprocessing training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz7M6FTU8hPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = PatternTokenizer()\n",
        "df[DATA_COLUMN] = df[DATA_COLUMN].map(lambda x: replace_contractions(x))\n",
        "df[DATA_COLUMN] = tokenizer.process_ds(df[DATA_COLUMN]).str.join(sep=\" \")\n",
        "df[DATA_COLUMN] = df[DATA_COLUMN].map(lambda x: remove_punctuation_3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYOjxea9DR6w",
        "colab_type": "text"
      },
      "source": [
        "preprocessing test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bheHuqnf9n8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test[DATA_COLUMN] = test[DATA_COLUMN].map(lambda x: replace_contractions(x))\n",
        "test[DATA_COLUMN] = tokenizer.process_ds(test[DATA_COLUMN]).str.join(sep=\" \")\n",
        "test[DATA_COLUMN] = test[DATA_COLUMN].map(lambda x: remove_punctuation_3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V2Gjg0HDUvZ",
        "colab_type": "text"
      },
      "source": [
        "y_train,x_test and x_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOSzaFw4_LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = df['finaltoxicity'].values\n",
        "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
        "X_train = df[\"comment_text\"].fillna(\"fillna\").values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6U94cgRDZG5",
        "colab_type": "text"
      },
      "source": [
        "intializing keras tokenizer-- this will create a vocabulary having max of 30000 words and assign a unique number to each word,\n",
        "converting train and test text into sequence of numbers and padding them \n",
        "max length for each sentence is 100 if the sentence is greater than 100 it will be truncated and if it is lesser than 100 it will be padded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAFS55RqL_jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94gklk7-ENvv",
        "colab_type": "text"
      },
      "source": [
        "filling null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmD93JBrbd52",
        "colab_type": "code",
        "outputId": "bb74a34a-8443-49c7-a149-695b692aacef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "X_train1 = train[\"comment_text\"].fillna(\"fillna\").values\n",
        "X_train1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now',\n",
              "       'daww he matches this background colour i am seemingly stuck with thanks talk january utc',\n",
              "       'hey man i am really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info',\n",
              "       ...,\n",
              "       'spitzer umm theres no actual article for prostitution ring crunch captain',\n",
              "       'and it looks like it was actually you who put on the speedy to have the first version deleted now that i look at it',\n",
              "       'and i really do not think you understand i came here and my idea was bad right away what kind of community goes you have bad ideas go away instead of helping rewrite them'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9stuy-eEQk5",
        "colab_type": "text"
      },
      "source": [
        "creating pickle file for tokenizer so that it can be used while prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlTJbYLA25A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv-Kw9uXEWyD",
        "colab_type": "text"
      },
      "source": [
        "creating an enmbedding vector using fast text and our text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFTC1sqtMB-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_jFj8tEc59",
        "colab_type": "text"
      },
      "source": [
        "saving the embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s1BHMM986TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# np.save('/content/drive/My Drive/DataSet/toxic_comment/x_test.np',x_test)\n",
        "\n",
        "np.save('/content/embedding_matrix',embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXf-KnqKEfT6",
        "colab_type": "text"
      },
      "source": [
        "writing the rocauc evaluation class which will give roc and auc value after each epoch training is completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S28scxZBOzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9zeBu_3Emr_",
        "colab_type": "text"
      },
      "source": [
        "creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YevdM0l4BHQy",
        "colab_type": "code",
        "outputId": "394ec8e6-b776-4491-fd91-822caa5e3a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "def get_model():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    outp = Dense(2, activation=\"softmax\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:200: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH-wlvFu7eD8",
        "colab_type": "code",
        "outputId": "2ffd2ceb-cc6f-4885-a4f3-3f95f3a59f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.0231    ,  0.017     ,  0.0157    , ...,  0.0744    ,\n",
              "        -0.1118    ,  0.0963    ],\n",
              "       [-0.0175    , -0.2189    ,  0.0353    , ..., -0.28459999,\n",
              "         0.0509    ,  0.0229    ],\n",
              "       ...,\n",
              "       [ 0.18269999, -0.2128    , -0.0233    , ..., -0.45300001,\n",
              "         0.12620001, -0.1129    ],\n",
              "       [-0.39930001, -0.454     ,  0.2484    , ..., -0.0574    ,\n",
              "        -0.3475    , -0.1999    ],\n",
              "       [-0.9497    , -0.0494    ,  0.1279    , ..., -0.0698    ,\n",
              "        -0.32429999,  0.08      ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpL4kUI9EqeF",
        "colab_type": "text"
      },
      "source": [
        "converitng the label into one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-WiHYFFClcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train = y_train.reshape(-1,1)\n",
        "enc.fit(y_train)\n",
        "y = enc.transform(y_train).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmNp4nyEXA-",
        "colab_type": "code",
        "outputId": "e7238f24-ae82-46b7-831a-2949f4b5b374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRB56oFBEv-Z",
        "colab_type": "text"
      },
      "source": [
        "training for 3 epoch with batch size 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIpG0sAEMIXP",
        "colab_type": "code",
        "outputId": "cd961f2a-e53e-4052-9fe0-291e1c1865c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 3\n",
        "\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y, train_size=0.85, random_state=233)\n",
        "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "\n",
        "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
        "                 callbacks=[RocAuc], verbose=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 135635 samples, validate on 23936 samples\n",
            "Epoch 1/3\n",
            "135635/135635 [==============================] - 523s 4ms/step - loss: 0.1049 - acc: 0.9608 - val_loss: 0.0989 - val_acc: 0.9633\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.979367 \n",
            "\n",
            "Epoch 2/3\n",
            "135635/135635 [==============================] - 521s 4ms/step - loss: 0.0751 - acc: 0.9714 - val_loss: 0.1046 - val_acc: 0.9629\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.979551 \n",
            "\n",
            "Epoch 3/3\n",
            " 41216/135635 [========>.....................] - ETA: 5:39 - loss: 0.0498 - acc: 0.9816"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egRIXItlE0E3",
        "colab_type": "text"
      },
      "source": [
        "doing prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBKA_fqzE1Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test, batch_size=1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUa63j8kW054",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"PooledGRUFastText.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_7sd50aW71X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_train[0:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31xcfsycrNCB",
        "colab_type": "code",
        "outputId": "32a8c65d-a183-41d4-ce55-d5dc07b0fab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "source": [
        "y_pred[0:30]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.99981403e-01, 2.00271606e-05],\n",
              "       [9.99986768e-01, 1.25467777e-05],\n",
              "       [9.97712493e-01, 2.74717808e-03],\n",
              "       [9.99994457e-01, 5.48362732e-06],\n",
              "       [9.99282002e-01, 8.45044851e-04],\n",
              "       [9.99985814e-01, 1.56760216e-05],\n",
              "       [2.95698643e-04, 9.99748826e-01],\n",
              "       [9.95862246e-01, 4.35298681e-03],\n",
              "       [9.98198509e-01, 1.71801448e-03],\n",
              "       [9.99960542e-01, 3.96370888e-05],\n",
              "       [9.99940753e-01, 5.16772270e-05],\n",
              "       [9.99951482e-01, 5.12301922e-05],\n",
              "       [1.11408055e-01, 8.93016696e-01],\n",
              "       [9.99827266e-01, 1.74492598e-04],\n",
              "       [9.97001171e-01, 2.49046087e-03],\n",
              "       [9.99253988e-01, 7.74383545e-04],\n",
              "       [8.44375193e-02, 9.19392824e-01],\n",
              "       [9.99995589e-01, 4.64916229e-06],\n",
              "       [9.99981105e-01, 2.26199627e-05],\n",
              "       [9.79566455e-01, 1.76667273e-02],\n",
              "       [9.99853134e-01, 1.82032585e-04],\n",
              "       [9.99108553e-01, 8.74966383e-04],\n",
              "       [9.99922514e-01, 7.29560852e-05],\n",
              "       [9.99999166e-01, 1.07288361e-06],\n",
              "       [9.99988556e-01, 1.06394291e-05],\n",
              "       [9.99993920e-01, 5.24520874e-06],\n",
              "       [9.99998689e-01, 1.46031380e-06],\n",
              "       [9.74089622e-01, 2.52657235e-02],\n",
              "       [9.99494255e-01, 5.36262989e-04],\n",
              "       [9.98146653e-01, 1.81558728e-03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTweg9d3MKyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHQi9zCLXvsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"PooledGRUFasttext.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4bC9t3YXx_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp PooledGRUFasttext.h5 '/content/drive/My Drive/DataSet/toxic_comment/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd9EOhECfkDC",
        "colab_type": "code",
        "outputId": "17e7dc85-aac2-40bc-ac77-16f1c22fd251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>finaltoxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>daww he matches this background colour i am se...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man i am really not trying to edit war its...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>more i can not make any real suggestions on im...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... finaltoxicity\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEkkI5FTkuTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('/content/newtrain.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZdaNXHalyRq",
        "colab_type": "code",
        "outputId": "b11cf10d-98a6-4fb4-b481-6a938c154acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>yo bitch ja rule is more succesful then you wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>from rfc the title is fine as it is imo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>sources zawe ashton on lapland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>if you have a look back at the source the info...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>i do not anonymously edit articles at all</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text\n",
              "0  00001cee341fdb12  yo bitch ja rule is more succesful then you wi...\n",
              "1  0000247867823ef7            from rfc the title is fine as it is imo\n",
              "2  00013b17ad220c46                     sources zawe ashton on lapland\n",
              "3  00017563c3f7919a  if you have a look back at the source the info...\n",
              "4  00017695ad8997eb          i do not anonymously edit articles at all"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE_CmjN6l1kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('/content/newtest.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE2hqdxXl-aS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}